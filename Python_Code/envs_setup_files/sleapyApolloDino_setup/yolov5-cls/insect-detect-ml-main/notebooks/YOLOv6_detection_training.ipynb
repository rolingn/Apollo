{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"NMauZMalbLl7"},"source":["<img src=\"https://raw.githubusercontent.com/maxsitt/insect-detect-docs/main/docs/assets/logo.png\" width=\"500\">\n","\n","# YOLOv6 detection model training for deployment on Luxonis OAK\n","\n","[![DOI](https://zenodo.org/badge/580963598.svg)](https://zenodo.org/badge/latestdoi/580963598)\n","[![License: AGPL v3](https://img.shields.io/badge/License-AGPL_v3-blue.svg)](https://choosealicense.com/licenses/agpl-3.0/)\n","\n","Author: &nbsp; Maximilian Sittinger &nbsp;\n","[<img src=\"https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png\" width=\"24\">](https://github.com/maxsitt) &nbsp;\n","[<img src=\"https://upload.wikimedia.org/wikipedia/commons/0/06/ORCID_iD.svg\" width=\"24\">](https://orcid.org/0000-0002-4096-8556)\n","\n","- [**Insect Detect Docs**](https://maxsitt.github.io/insect-detect-docs/) ðŸ“‘\n","- [`insect-detect-ml`](https://github.com/maxsitt/insect-detect-ml) GitHub repo\n","\n","&nbsp;\n","\n","**Train a [YOLOv6](https://github.com/meituan/YOLOv6) object detection model on your own custom dataset!**\n","\n","- Go to **File** in the top menu bar and choose **Save a copy in Drive** before running the notebook.\n","- Go to **Runtime** and make sure that **GPU** is selected as Hardware accelerator under **Change runtime type**.\n","- If you are using Firefox, please make sure to allow notifications for this website.\n","- Using dataset import from [Roboflow](https://roboflow.com/) is recommended, but is not required.\n","> Choose option [`Upload dataset from Google Drive`](#scrollTo=RxOnnOadc5vR) instead.\n","- Connecting to Google Drive is recommended, but is not required.\n","> Choose option [`Upload dataset from your local file system`](#scrollTo=qKTCWdtkOUw7) (slower!) and [`Download results`](#scrollTo=h90_4rFQx0mp) instead.\n","\n","&nbsp;\n","\n","---\n","\n","**References**\n","\n","1. Official YOLOv6 tutorial notebook by MeiTuan &emsp;\n","[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/meituan/YOLOv6/blob/main/turtorial.ipynb)\n","2. Roboflow tutorial notebook for YOLOv6 training &nbsp;\n","[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-yolov6-object-detection-on-custom-data.ipynb)\n","3. DepthAI tutorial notebook for YOLOv6 training &emsp;\n","[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/luxonis/depthai-ml-training/blob/master/colab-notebooks/YoloV6_training.ipynb)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"3q8IjnB8QGG3"},"source":["# Initialization"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"pbdTb-q4QJ0M"},"source":["## Show GPU + CPU and Linux distribution"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MsJ9oBX1dUhh"},"outputs":[],"source":["!nvidia-smi -L\n","print(\"\\nCPU:\")\n","!grep \"model name\" /proc/cpuinfo\n","print(\"\\nLinux distribution:\")\n","!grep \"PRETTY_NAME\" /etc/os-release"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"wVi4j857p0GS"},"source":["## YOLOv6 setup"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P1F_lwcyqItL"},"outputs":[],"source":["!git clone https://github.com/meituan/YOLOv6\n","%cd /content/YOLOv6\n","\n","# Delete numpy from requirements.txt to avoid version conflicts (already installed in Google Colab)\n","!sed -i \"/numpy/d\" requirements.txt\n","%pip install -qr requirements.txt"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"g41kdogczKts"},"source":["## Upload dataset from Roboflow\n","\n","If you are not sure how to export your annotated dataset, check the [Roboflow docs](https://docs.roboflow.com/exporting-data).\n","\n","> Alternatively you can upload your dataset ([YOLOv6 format](https://github.com/meituan/YOLOv6/blob/main/docs/Train_custom_data.md#1-prepare-your-own-dataset)) from [**Google Drive**](#scrollTo=RxOnnOadc5vR) or from your [**local file system**](#scrollTo=qKTCWdtkOUw7) in the next steps."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aRSmjO9MQRVq"},"outputs":[],"source":["%pip install -q roboflow"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"8cqOYoopQx-U"},"source":["**Copy only the last three lines of your Download Code and insert them in the next code cell:**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0SsPwQDzRvwH"},"outputs":[],"source":["from pathlib import Path\n","from roboflow import Roboflow\n","\n","%cd /content/YOLOv6\n","\n","### Paste your Download Code here:\n","rf = Roboflow(api_key=\"XXXXXXXXXXXXXXXXXXXX\")\n","project = rf.workspace(\"maximilian-sittinger\").project(\"insect_detect_detection\")\n","dataset = project.version(7).download(\"mt-yolov6\")\n","###\n","\n","dataset_location = dataset.location\n","\n","print(f\"\\nLocation of dataset: {dataset_location}\")\n","print(f\"\\nTotal number of images: {len(list(Path(dataset_location).glob('**/*.jpg')))}\")\n","\n","if Path(f\"{dataset_location}/images/train\").exists():\n","  print(f\"\\nNumber of training images: {len(list(Path(f'{dataset_location}/images/train').glob('*.jpg')))}\")\n","if Path(f\"{dataset_location}/images/valid\").exists():\n","  print(f\"Number of validation images: {len(list(Path(f'{dataset_location}/images/valid').glob('*.jpg')))}\")\n","if Path(f\"{dataset_location}/images/test\").exists():\n","  print(f\"Number of test images: {len(list(Path(f'{dataset_location}/images/test').glob('*.jpg')))}\")\n","print(\"\\nContent of data.yaml file:\")\n","%cat {dataset_location}/data.yaml"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"RxOnnOadc5vR"},"source":["## Recommended: Connect to Google Drive"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q4lMoPNddCtx"},"outputs":[],"source":["from google.colab import drive\n","drive.mount(\"/content/drive\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hFA-ROJ8rUWU"},"outputs":[],"source":["#@title ## Upload dataset from Google Drive {display-mode: \"form\"}\n","\n","#@markdown ### Google Drive path to your (zipped) dataset folder:\n","dataset_path = \"/content/drive/MyDrive/yolov6_dataset.zip\" #@param {type: \"string\"}\n","#@markdown - Please make sure to compress your dataset folder to **.zip** file for much faster upload speed!\n","#@markdown - Dataset has to be in [YOLOv6 format](https://github.com/meituan/YOLOv6/blob/main/docs/Train_custom_data.md#1-prepare-your-own-dataset).\n","\n","from pathlib import Path\n","\n","dataset_location = f\"/content/YOLOv6/{Path(dataset_path).stem}\"\n","\n","print(\"Uploading dataset from Google Drive...\\n\")\n","!rsync -ah --info=progress2 --no-i-r {dataset_path} /content/YOLOv6\n","if Path(dataset_path).suffix == \".zip\":\n","  import zipfile\n","  zip_path = f\"/content/YOLOv6/{Path(dataset_path).stem}.zip\"\n","  if len(list(zipfile.Path(zip_path).iterdir())) > 1:\n","    !unzip -uq {zip_path} -d {dataset_location}\n","  else:\n","    !unzip -uq {zip_path} -d /content/YOLOv6\n","  %rm {zip_path}\n","print(\"\\nDataset was successfully uploaded!\")\n","\n","print(f\"\\nLocation of dataset: {dataset_location}\")\n","print(f\"\\nTotal number of images: {len(list(Path(dataset_location).glob('**/*.jpg')))}\")\n","\n","if Path(f\"{dataset_location}/images/train\").exists():\n","  print(f\"\\nNumber of training images: {len(list(Path(f'{dataset_location}/images/train').glob('*.jpg')))}\")\n","if Path(f\"{dataset_location}/images/valid\").exists():\n","  print(f\"Number of validation images: {len(list(Path(f'{dataset_location}/images/valid').glob('*.jpg')))}\")\n","if Path(f\"{dataset_location}/images/test\").exists():\n","  print(f\"Number of test images: {len(list(Path(f'{dataset_location}/images/test').glob('*.jpg')))}\")\n","print(\"\\nContent of data.yaml file:\")\n","%cat {dataset_location}/data.yaml"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qKTCWdtkOUw7"},"outputs":[],"source":["#@title ## Upload dataset from your local file system {display-mode: \"form\"}\n","\n","#@markdown ### Name of your zipped dataset folder:\n","dataset_name = \"yolov6_dataset\" #@param {type: \"string\"}\n","#@markdown - Please make sure to compress your dataset folder to **.zip** file before uploading!\n","#@markdown - The name of the .zip file should be the same as for the dataset folder.\n","#@markdown - Dataset has to be in [YOLOv6 format](https://github.com/meituan/YOLOv6/blob/main/docs/Train_custom_data.md#1-prepare-your-own-dataset).\n","\n","from pathlib import Path\n","import zipfile\n","from google.colab import files\n","\n","dataset_location = f\"/content/YOLOv6/{dataset_name}\"\n","\n","uploaded = files.upload()\n","\n","if len(list(zipfile.Path(f\"{dataset_name}.zip\").iterdir())) > 1:\n","  !unzip -uq {dataset_name}.zip -d {dataset_location}\n","else:\n","  !unzip -uq {dataset_name}.zip -d /content/YOLOv6\n","%rm {dataset_name}.zip\n","\n","print(f\"\\nLocation of dataset: {dataset_location}\")\n","print(f\"\\nTotal number of images: {len(list(Path(dataset_location).glob('**/*.jpg')))}\")\n","\n","if Path(f\"{dataset_location}/images/train\").exists():\n","  print(f\"\\nNumber of training images: {len(list(Path(f'{dataset_location}/images/train').glob('*.jpg')))}\")\n","if Path(f\"{dataset_location}/images/valid\").exists():\n","  print(f\"Number of validation images: {len(list(Path(f'{dataset_location}/images/valid').glob('*.jpg')))}\")\n","if Path(f\"{dataset_location}/images/test\").exists():\n","  print(f\"Number of test images: {len(list(Path(f'{dataset_location}/images/test').glob('*.jpg')))}\")\n","print(\"\\nContent of data.yaml file:\")\n","%cat {dataset_location}/data.yaml"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"uv36pkEtMupF"},"source":["## Edit `data.yaml`\n","\n","Check the `data.yaml` file in your dataset folder to make sure the paths to the train, valid and test folders are correct.\n","\n","- Open your dataset folder in the File Explorer (Folder symbol on the left side bar).\n","- Double-click on the `data.yaml` file, it will open in the editor to the right.\n","\n","  Make sure that the paths to the train, valid and test folders are as follows:\n","\n","  ``` yaml\n","  train: <DATASET_NAME>/images/train\n","  val: <DATASET_NAME>/images/valid\n","  test: <DATASET_NAME>/images/test\n","  ```\n","\n","- Insert the correct name of your dataset folder at `<DATASET_NAME>`.\n","- Save your changes with **Ctrl + S** and close the editor."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"nnn4pSbI6eTv"},"source":["# Model training"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"U4t4JhGYGOcr"},"source":["## Tensorboard logger\n","\n","> If you are using Firefox, **disable Enhanced Tracking Protection** for this website (click on the shield to the left of the address bar) for the Tensorboard logger to work correctly!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lYCUyGITGU6j"},"outputs":[],"source":["%load_ext tensorboard\n","%tensorboard --logdir /content/YOLOv6/runs/train"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"7t6PUcz3SsEy"},"source":["## Train YOLOv6 detection model\n","\n","- `--name` name of the training run\n","- `--img` input image size (recommended: same size as for inference)\n","- `--batch` specify batch size (recommended: 32)\n","- `--epochs` set the number of training [epochs](https://machine-learning.paperspace.com/wiki/epoch) (recommended: 100-300+)\n","- `--data` path to `data.yaml` file\n","- `--conf` specify the model configuration file ([pretrained model weights](https://github.com/meituan/YOLOv6#benchmark))\n","> `--conf configs/yolov6n_finetune.py` YOLOv6-N model (recommended)  \n","  `--conf configs/yolov6s_finetune.py` YOLOv6-S model\n","\n","> More information on YOLOv6 [model training](https://blog.roboflow.com/how-to-train-yolov6-on-a-custom-dataset/#configure-yolov6-custom-training-options) ðŸš€"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qAYNJg9M7sg3"},"outputs":[],"source":["training_run_name = \"YOLOv6n_320_batch32_epochs200\" #@param {type: \"string\"}\n","#@markdown Add UTC timestamp in front of training run name:\n","add_timestamp = True #@param {type:\"boolean\"}\n","#@markdown ---\n","\n","image_size = 320 #@param {type: \"integer\"}\n","batch_size = 32 #@param {type:\"slider\", min:32, max:128, step:32}\n","number_epochs = 200 #@param {type:\"slider\", min:10, max:500, step:10}\n","config = \"yolov6n_finetune.py\" #@param [\"yolov6n_finetune.py\", \"yolov6s_finetune.py\"]\n","\n","if add_timestamp:\n","  from datetime import datetime\n","  utc_timestamp = datetime.now().strftime(\"%Y%m%d_%H-%M\")\n","  train_run_name = f\"{utc_timestamp}_{training_run_name}\"\n","else:\n","  train_run_name = training_run_name\n","\n","%cd /content/YOLOv6\n","\n","!wget -q https://github.com/meituan/YOLOv6/releases/download/0.4.0/yolov6n.pt -P weights/\n","!wget -q https://github.com/meituan/YOLOv6/releases/download/0.4.0/yolov6s.pt -P weights/\n","\n","!python tools/train.py \\\n","--name {train_run_name} \\\n","--img {image_size} \\\n","--batch {batch_size} \\\n","--epochs {number_epochs} \\\n","--data {dataset_location}/data.yaml \\\n","--conf configs/{config}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h90_4rFQx0mp"},"outputs":[],"source":["#@title ## Export to Google Drive or Download training results {display-mode: \"form\"}\n","\n","training_results = \"Export_Google_Drive\" #@param [\"Export_Google_Drive\", \"Download\"]\n","#@markdown - Open `YOLOv6/runs/train/{train_run_name}` in the File Explorer (Folder symbol on the left\n","#@markdown side bar) and delete the `events.out.tfevents` file to decrease the size of your train folder.\n","#@markdown ---\n","\n","#@markdown ### Path for saving training results in Google Drive:\n","GDrive_save_path = \"/content/drive/MyDrive/Training_results/YOLOv6\" #@param {type: \"string\"}\n","\n","if training_results == \"Export_Google_Drive\":\n","  print(\"Exporting training results to Google Drive...\\n\")\n","  !rsync -ah --mkpath --info=progress2 --no-i-r /content/YOLOv6/runs/train/{train_run_name} {GDrive_save_path}\n","  print(\"\\nTraining results were successfully exported!\")\n","elif training_results == \"Download\":\n","  from google.colab import files\n","  %cd /content/YOLOv6/runs/train\n","  !zip -rq {train_run_name}.zip {train_run_name}\n","  %cd -\n","  files.download(f\"/content/YOLOv6/runs/train/{train_run_name}.zip\")"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"k54rL7jSM_ni"},"source":["# Model validation\n","\n","Test the performance of your model on the validation and/or test dataset.\n","\n","> Copy the validation results (cell output) and save to .txt file, as they will not be saved automatically."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pPKGJ1k8NDmo"},"outputs":[],"source":["task = \"val\" #@param [\"val\", \"test\"]\n","#@markdown > Use `task: test` to validate on the dataset test split.\n","\n","val_run_name = f\"{train_run_name}_validate_{task}\"\n","\n","%cd /content/YOLOv6\n","\n","!python tools/eval.py \\\n","--name {val_run_name} \\\n","--weights runs/train/{train_run_name}/weights/best_ckpt.pt \\\n","--data {dataset_location}/data.yaml \\\n","--img {image_size} \\\n","--do_pr_metric True \\\n","--plot_confusion_matrix \\\n","--task {task}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z-o3qi70mGTW"},"outputs":[],"source":["#@title ## Export to Google Drive or Download validation results {display-mode: \"form\"}\n","\n","validation_results = \"Export_Google_Drive\" #@param [\"Export_Google_Drive\", \"Download\"]\n","#@markdown ---\n","\n","#@markdown ### Path for saving validation results in Google Drive:\n","GDrive_save_path = \"/content/drive/MyDrive/Training_results/YOLOv6\" #@param {type: \"string\"}\n","\n","if validation_results == \"Export_Google_Drive\":\n","  print(\"Exporting validation results to Google Drive...\\n\")\n","  !rsync -ah --mkpath --info=progress2 --no-i-r /content/YOLOv6/runs/val/{val_run_name} {GDrive_save_path}/{train_run_name}\n","  print(\"\\nValidation results were successfully exported!\")\n","elif validation_results == \"Download\":\n","  from google.colab import files\n","  %cd /content/YOLOv6/runs/val\n","  !zip -rq {val_run_name}.zip {val_run_name}\n","  %cd -\n","  files.download(f\"/content/YOLOv6/runs/val/{val_run_name}.zip\")"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"Rs7bZGuQROhk"},"source":["# Model inference\n","\n","Use your model to detect insects on images in the dataset test split."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T99Qs1noRTbb"},"outputs":[],"source":["#@markdown #### Decrease confidence threshold to detect objects with lower confidence score:\n","confidence_threshold = 0.5 #@param {type:\"slider\", min:0.1, max:1, step:0.1}\n","#@markdown #### Increase IoU threshold if the same object is detected multiple times:\n","iou_threshold = 0.5 #@param {type:\"slider\", min:0.1, max:1, step:0.1}\n","\n","det_run_name = f\"{train_run_name}_detect\"\n","\n","%cd /content/YOLOv6\n","\n","!python tools/infer.py \\\n","--name {det_run_name} \\\n","--weights runs/train/{train_run_name}/weights/best_ckpt.pt \\\n","--yaml {dataset_location}/data.yaml \\\n","--source {dataset_location}/images/test/ \\\n","--img {image_size} {image_size} \\\n","--conf-thres {confidence_threshold} \\\n","--iou-thres {iou_threshold}"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#@title ## Export to Google Drive or Download inference results {display-mode: \"form\"}\n","\n","inference_results = \"Export_Google_Drive\" #@param [\"Export_Google_Drive\", \"Download\"]\n","#@markdown ---\n","\n","#@markdown ### Path for saving inference results in Google Drive:\n","GDrive_save_path = \"/content/drive/MyDrive/Training_results/YOLOv6\" #@param {type: \"string\"}\n","\n","%cd /content/YOLOv6/runs/inference\n","!zip -rq {det_run_name}.zip {det_run_name}\n","%cd -\n","\n","if inference_results == \"Export_Google_Drive\":\n","  print(\"\\nExporting inference results to Google Drive...\\n\")\n","  !rsync -ah --mkpath --info=progress2 --no-i-r /content/YOLOv6/runs/inference/{det_run_name}.zip {GDrive_save_path}/{train_run_name}\n","  print(\"\\nInference results were successfully exported!\")\n","elif inference_results == \"Download\":\n","  from google.colab import files\n","  files.download(f\"/content/YOLOv6/runs/inference/{det_run_name}.zip\")"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"3_XwmhjIXnFt"},"source":["## Show inference results on test images"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FsskeC4jRrj8"},"outputs":[],"source":["from pathlib import Path\n","from IPython.display import Image, display\n","\n","for img in Path(f\"/content/YOLOv6/runs/inference/{det_run_name}\").glob(\"*.jpg\"):\n","  display(Image(img))\n","  print(\"\\n\")"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"KiU6Qt79ShXJ"},"source":["# Model conversion\n","\n","**Go to [tools.luxonis.com](https://tools.luxonis.com/):**\n","\n","- Select `YoloV6 (latest)` as Yolo Version.\n","- Rename your model weights file from `best_ckpt.pt` to e.g. `yolov6n_320.pt`.\n","- Select your model weights file (`yolov6n_320.pt`) for upload.\n","- Use your image size (e.g. `320`) as input image shape.\n","- Open the `Advanced options` and choose `Shaves: 4`.\n","- Hit `Submit` to upload your PyTorch model weights and download the converted ONNX, OpenVINO and .blob model.\n","\n","> Follow the instructions in the README of the [`luxonis/tools`](https://github.com/luxonis/tools) GitHub repo to run the model conversion locally.\n","\n","---\n","\n","Recommended number of shaves the model can use is **4-5** for deployment with the Insect Detect camera trap.\n","\n","> More information about SHAVES can be found at the [DepthAI FAQ](https://docs.luxonis.com/en/latest/pages/faq/#what-are-the-shaves).\n","\n","> More information on model conversion can be found at the [DepthAI Docs](https://docs.luxonis.com/en/latest/pages/model_conversion/)."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"EPlXQe3lQPsi"},"source":["## Generate JSON config file\n","\n","Together with the converted .blob model, a .json config file with model specific settings will be created after following the conversion steps at [tools.luxonis.com](https://tools.luxonis.com/).\n","\n","To set the correct class/label name(s) and adjust the confidence or IoU threshold, you can change these values directly in the .json file you downloaded from tools.luxonis.com or create your own .json config file in the following step."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u80iHmR7QSd-"},"outputs":[],"source":["#@markdown ### Name of the JSON config file:\n","json_name = \"yolov6_320\" #@param {type: \"string\"}\n","#@markdown ---\n","\n","image_size = 320 #@param {type: \"integer\"}\n","number_classes = 1 #@param {type: \"integer\"}\n","#@markdown ---\n","\n","#@markdown #### For several classes/labels: **[\"class1\", \"class2\", \"class3\"]**\n","labels = [\"insect\"] #@param {type: \"raw\"}\n","#@markdown ---\n","\n","#@markdown #### Decrease confidence threshold to detect objects with lower confidence score:\n","confidence_threshold = 0.5 #@param {type:\"slider\", min:0.1, max:1, step:0.1}\n","#@markdown #### Increase IoU threshold if the same object is detected multiple times:\n","iou_threshold = 0.5 #@param {type:\"slider\", min:0.1, max:1, step:0.1}\n","\n","import json\n","from google.colab import files\n","\n","!wget -q https://raw.githubusercontent.com/luxonis/depthai-experiments/master/gen2-yolo/device-decoding/json/yolov5.json -P /content/\n","\n","with open(\"/content/yolov5.json\", \"r\") as json_template:\n","  json_data = json.load(json_template)\n","  json_data[\"nn_config\"][\"input_size\"] = f\"{image_size}x{image_size}\"\n","  json_data[\"nn_config\"][\"NN_specific_metadata\"][\"classes\"] = number_classes\n","  json_data[\"nn_config\"][\"NN_specific_metadata\"][\"anchors\"] = []\n","  json_data[\"nn_config\"][\"NN_specific_metadata\"][\"anchor_masks\"] = {}\n","  json_data[\"nn_config\"][\"NN_specific_metadata\"][\"iou_threshold\"] = iou_threshold\n","  json_data[\"nn_config\"][\"NN_specific_metadata\"][\"confidence_threshold\"] = confidence_threshold\n","  json_data[\"mappings\"][\"labels\"] = labels\n","\n","with open(f\"/content/{json_name}.json\", \"w\") as json_file:\n","  json.dump(json_data, json_file, indent = 4)\n","\n","files.download(f\"/content/{json_name}.json\")"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"g_SAn96loGK-"},"source":["# Model deployment\n","\n","That's it! You trained your own [YOLOv6](https://github.com/meituan/YOLOv6) object detection model with your custom dataset and converted it to .blob format which is necessary to run inference on the [Luxonis OAK devices](https://docs.luxonis.com/projects/hardware/en/latest/).\n","\n","> To deploy the YOLOv6 model on your OAK device you can check out the Luxonis GitHub repository for [on-device decoding](https://github.com/luxonis/depthai-experiments/tree/master/gen2-yolo/device-decoding) or use the deployment options from the [**Insect Detect Docs**](https://maxsitt.github.io/insect-detect-docs/software/programming/) (e.g. for continuous automated insect monitoring with the DIY camera trap)."]}],"metadata":{"accelerator":"GPU","colab":{"private_outputs":true,"provenance":[{"file_id":"https://github.com/maxsitt/insect-detect-ml/blob/main/notebooks/YOLOv5_detection_training_OAK_conversion.ipynb","timestamp":1678184163562},{"file_id":"1CDz0HUpYTTLxmvPpCx5b2nVyjY_LIYQ6","timestamp":1671662125129}]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.1 (tags/v3.11.1:a7a450f, Dec  6 2022, 19:58:39) [MSC v.1934 64 bit (AMD64)]"},"vscode":{"interpreter":{"hash":"baf2788c67905bf5eabce425833f665485fde887eca8cd7474f373ca3e9af677"}}},"nbformat":4,"nbformat_minor":0}
