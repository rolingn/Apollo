{"cells":[{"cell_type":"markdown","metadata":{"id":"NMauZMalbLl7"},"source":["<img src=\"https://raw.githubusercontent.com/maxsitt/insect-detect-docs/main/docs/assets/logo.png\" width=\"500\">\n","\n","# YOLOv5 classification model training + ONNX export\n","\n","[![DOI](https://zenodo.org/badge/580963598.svg)](https://zenodo.org/badge/latestdoi/580963598)\n","[![License: AGPL v3](https://img.shields.io/badge/License-AGPL_v3-blue.svg)](https://choosealicense.com/licenses/agpl-3.0/)\n","\n","Author: &nbsp; Maximilian Sittinger &nbsp;\n","[<img src=\"https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png\" width=\"24\">](https://github.com/maxsitt) &nbsp;\n","[<img src=\"https://upload.wikimedia.org/wikipedia/commons/0/06/ORCID_iD.svg\" width=\"24\">](https://orcid.org/0000-0002-4096-8556)\n","\n","- [**Insect Detect Docs**](https://maxsitt.github.io/insect-detect-docs/) ðŸ“‘\n","- [`insect-detect-ml`](https://github.com/maxsitt/insect-detect-ml) GitHub repo\n","\n","&nbsp;\n","\n","**Train an image classification model on your own custom dataset with [YOLOv5](https://github.com/ultralytics/yolov5#classification)!**\n","\n","- Go to **File** in the top menu bar and choose **Save a copy in Drive** before running the notebook.\n","- Go to **Runtime** and make sure that **GPU** is selected as Hardware accelerator under **Change runtime type**.\n","- If you are using Firefox, please make sure to allow notifications for this website.\n","- Using dataset import from [Roboflow](https://roboflow.com/) compresses the images which can lead to a decreased model accuracy.\n","> Choose option [`Upload dataset from Google Drive`](#scrollTo=hFA-ROJ8rUWU) or `Upload dataset from Zenodo` instead.\n","- Connecting to Google Drive is recommended, but is not required.\n","> Choose option [`Upload dataset from your local file system`](#scrollTo=qKTCWdtkOUw7) (slower!) and [`Download results`](#scrollTo=h90_4rFQx0mp) instead.\n","\n","&nbsp;\n","\n","---\n","\n","**References**\n","\n","1. Official YOLOv5 classification tutorial notebook by Ultralytics &nbsp;\n","[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ultralytics/yolov5/blob/master/classify/tutorial.ipynb)\n","1. Roboflow tutorial notebook for YOLOv5 classification training &nbsp;\n","[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-yolov5-classification-on-custom-data.ipynb)"]},{"cell_type":"markdown","metadata":{"id":"3q8IjnB8QGG3"},"source":["# Initialization"]},{"cell_type":"markdown","metadata":{"id":"pbdTb-q4QJ0M"},"source":["## Show GPU + CPU and Linux distribution"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MsJ9oBX1dUhh"},"outputs":[],"source":["!nvidia-smi -L\n","print(\"\\nCPU:\")\n","!grep \"model name\" /proc/cpuinfo\n","print(\"\\nLinux distribution:\")\n","!grep \"PRETTY_NAME\" /etc/os-release"]},{"cell_type":"markdown","metadata":{"id":"wVi4j857p0GS"},"source":["## YOLOv5 setup"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P1F_lwcyqItL"},"outputs":[],"source":["!git clone https://github.com/maxsitt/yolov5 # custom YOLOv5 fork\n","%cd /content/yolov5\n","\n","# Delete onnxruntime from requirements.txt\n","!sed -i \"/onnxruntime/d\" requirements.txt\n","\n","%pip install -qr requirements.txt\n","\n","# Don't use the package albumentations for image augmentations (installed by default in Google Colab)\n","# -> images will only be resized (opencv): https://github.com/maxsitt/yolov5/blob/master/utils/augmentations.py#L356-L370\n","# -> these augmentations will not be used: https://github.com/maxsitt/yolov5/blob/master/utils/augmentations.py#L312-L348\n","%pip uninstall -y albumentations\n","\n","import torch\n","import utils\n","\n","# Install onnxruntime-gpu if CUDA is available or onnxruntime for CPU inference\n","if torch.cuda.is_available():\n","  %pip install -q onnxruntime-gpu\n","else:\n","  %pip install -q onnxruntime\n","\n","display = utils.notebook_init()"]},{"cell_type":"markdown","metadata":{"id":"RxOnnOadc5vR"},"source":["## Recommended: Connect to Google Drive"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q4lMoPNddCtx"},"outputs":[],"source":["from google.colab import drive\n","drive.mount(\"/content/drive\")"]},{"cell_type":"markdown","metadata":{"id":"r-fPrLUotTtR"},"source":["## Folder structure of your classification dataset\n","\n","Separating the dataset into a training (\"train\"), validation (\"val\") and test split is necessary to correctly evaluate the performance of your model. A split ratio of 70% train, 20% val and 10% test is recommended. You can find more info in this [blog article](https://blog.roboflow.com/train-test-split/).\n","\n","> You can upload your original dataset and [**split it into train/val/test subsets**](#scrollTo=vkr0vBcOlT-t) in one of the following steps before training!\n","\n","```\n","dataset_name\n","â”œâ”€â”€ train\n","â”‚Â Â  â”œâ”€â”€ class_1\n","â”‚Â Â  â”‚Â Â  â”œâ”€â”€ IMG_123.jpg\n","â”‚Â Â  â””â”€â”€ class_2\n","â”‚Â Â      â”œâ”€â”€ IMG_456.jpg\n","â”œâ”€â”€ val\n","â”‚Â Â  â”œâ”€â”€ class_1\n","â”‚Â Â  â”‚Â Â  â”œâ”€â”€ IMG_789.jpg\n","â”‚Â Â  â””â”€â”€ class_2\n","â”‚Â Â      â”œâ”€â”€ IMG_101.jpg\n","â”œâ”€â”€ test\n","â”‚Â Â  â”œâ”€â”€ class_1\n","â”‚Â Â  â”‚Â Â  â”œâ”€â”€ IMG_121.jpg\n","â”‚Â Â  â””â”€â”€ class_2\n","â”‚Â Â      â”œâ”€â”€ IMG_341.jpg\n","```"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hFA-ROJ8rUWU"},"outputs":[],"source":["#@title ## Upload dataset from Google Drive {display-mode: \"form\"}\n","\n","#@markdown ### Google Drive path to your (zipped) dataset folder:\n","dataset_path = \"/content/drive/MyDrive/classification_dataset.zip\" #@param {type: \"string\"}\n","#@markdown - Please make sure to compress your dataset folder to **.zip** file for much faster upload speed!\n","\n","from pathlib import Path\n","\n","dataset_location = f\"/content/yolov5/{Path(dataset_path).stem}\"\n","\n","print(\"Uploading dataset from Google Drive...\\n\")\n","!rsync -ah --info=progress2 --no-i-r {dataset_path} /content/yolov5\n","if Path(dataset_path).suffix == \".zip\":\n","  import zipfile\n","  zip_path = f\"/content/yolov5/{Path(dataset_path).stem}.zip\"\n","  if len(list(zipfile.Path(zip_path).iterdir())) > 1:\n","    !unzip -uq {zip_path} -d {dataset_location}\n","  else:\n","    !unzip -uq {zip_path} -d /content/yolov5\n","  %rm {zip_path}\n","print(\"\\nDataset was successfully uploaded!\")\n","\n","if Path(f\"{dataset_location}/valid\").exists():\n","  Path(f\"{dataset_location}/valid\").rename(f\"{dataset_location}/val\")\n","\n","print(f\"\\nLocation of dataset: {dataset_location}\")\n","print(f\"\\nTotal number of images: {len(list(Path(dataset_location).glob('**/*.jpg')))}\")\n","\n","if Path(f\"{dataset_location}/train\").exists():\n","  classes_train = sorted(list(Path(f\"{dataset_location}/train\").glob(\"*\")))\n","  print(f\"\\nNumber of classes: {len(classes_train)}\")\n","  print(f\"\\nNumber of training images: {len(list(Path(f'{dataset_location}/train').glob('**/*.jpg')))}\")\n","  print(\"Number of images per class in the train split:\")\n","  print(\"\\n\".join(f\"{c.name}: {len(list((Path(c).glob('*.jpg'))))}\" for c in classes_train))\n","else:\n","  classes = sorted(list(Path(dataset_location).glob(\"*\")))\n","  print(f\"\\nNumber of classes: {len(classes)}\")\n","  print(\"\\nNumber of images per class:\")\n","  print(\"\\n\".join(f\"{c.name}: {len(list((Path(c).glob('*.jpg'))))}\" for c in classes))\n","if Path(f\"{dataset_location}/val\").exists():\n","  classes_val = sorted(list(Path(f\"{dataset_location}/val\").glob(\"*\")))\n","  print(f\"\\nNumber of validation images: {len(list(Path(f'{dataset_location}/val').glob('**/*.jpg')))}\")\n","  print(\"Number of images per class in the val split:\")\n","  print(\"\\n\".join(f\"{c.name}: {len(list((Path(c).glob('*.jpg'))))}\" for c in classes_val))\n","if Path(f\"{dataset_location}/test\").exists():\n","  classes_test = sorted(list(Path(f\"{dataset_location}/test\").glob(\"*\")))\n","  print(f\"\\nNumber of test images: {len(list(Path(f'{dataset_location}/test').glob('**/*.jpg')))}\")\n","  print(\"Number of images per class in the test split:\")\n","  print(\"\\n\".join(f\"{c.name}: {len(list((Path(c).glob('*.jpg'))))}\" for c in classes_test))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#@title ## Upload dataset from Zenodo {display-mode: \"form\"}\n","\n","#@markdown ### Zenodo DOI of dataset:\n","zenodo_doi = \"10.5281/zenodo.8325384\" #@param {type: \"string\"}\n","#@markdown - Only works with single dataset folder or .zip file in the Zenodo record.\n","\n","%pip install -q zenodo_get\n","\n","from pathlib import Path\n","import zenodo_get\n","\n","print(\"\\n\")\n","!zenodo_get {zenodo_doi} --output-dir /content/zenodo\n","\n","dataset_folder = [f for f in Path(\"/content/zenodo\").iterdir() if f.is_dir()]\n","if len(dataset_folder) > 1:\n","  print(\"\\nFound more than one dataset folder!\")\n","elif len(dataset_folder) == 1:\n","  dataset_location = f\"/content/yolov5/{dataset_folder[0]}\"\n","  Path(dataset_folder[0]).rename(dataset_location)\n","else:\n","  import zipfile\n","  dataset_zip = list(Path(\"/content/zenodo\").glob(\"*.zip\"))\n","  if len(dataset_zip) > 1:\n","    print(\"\\nFound more than one dataset .zip file!\")\n","  else:\n","    zip_path = dataset_zip[0]\n","    dataset_location = f\"/content/yolov5/{zip_path.stem}\"\n","    if len(list(zipfile.Path(zip_path).iterdir())) > 1:\n","      !unzip -uq {zip_path} -d {dataset_location}\n","    else:\n","      !unzip -uq {zip_path} -d /content/yolov5\n","    %rm {zip_path}\n","print(\"\\nDataset was successfully uploaded!\")\n","\n","if Path(f\"{dataset_location}/valid\").exists():\n","  Path(f\"{dataset_location}/valid\").rename(f\"{dataset_location}/val\")\n","\n","print(f\"\\nLocation of dataset: {dataset_location}\")\n","print(f\"\\nTotal number of images: {len(list(Path(dataset_location).glob('**/*.jpg')))}\")\n","\n","if Path(f\"{dataset_location}/train\").exists():\n","  classes_train = sorted(list(Path(f\"{dataset_location}/train\").glob(\"*\")))\n","  print(f\"\\nNumber of classes: {len(classes_train)}\")\n","  print(f\"\\nNumber of training images: {len(list(Path(f'{dataset_location}/train').glob('**/*.jpg')))}\")\n","  print(\"Number of images per class in the train split:\")\n","  print(\"\\n\".join(f\"{c.name}: {len(list((Path(c).glob('*.jpg'))))}\" for c in classes_train))\n","else:\n","  classes = sorted(list(Path(dataset_location).glob(\"*\")))\n","  print(f\"\\nNumber of classes: {len(classes)}\")\n","  print(\"\\nNumber of images per class:\")\n","  print(\"\\n\".join(f\"{c.name}: {len(list((Path(c).glob('*.jpg'))))}\" for c in classes))\n","if Path(f\"{dataset_location}/val\").exists():\n","  classes_val = sorted(list(Path(f\"{dataset_location}/val\").glob(\"*\")))\n","  print(f\"\\nNumber of validation images: {len(list(Path(f'{dataset_location}/val').glob('**/*.jpg')))}\")\n","  print(\"Number of images per class in the val split:\")\n","  print(\"\\n\".join(f\"{c.name}: {len(list((Path(c).glob('*.jpg'))))}\" for c in classes_val))\n","if Path(f\"{dataset_location}/test\").exists():\n","  classes_test = sorted(list(Path(f\"{dataset_location}/test\").glob(\"*\")))\n","  print(f\"\\nNumber of test images: {len(list(Path(f'{dataset_location}/test').glob('**/*.jpg')))}\")\n","  print(\"Number of images per class in the test split:\")\n","  print(\"\\n\".join(f\"{c.name}: {len(list((Path(c).glob('*.jpg'))))}\" for c in classes_test))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qKTCWdtkOUw7"},"outputs":[],"source":["#@title ## Upload dataset from your local file system {display-mode: \"form\"}\n","\n","#@markdown ### Name of your zipped dataset folder:\n","dataset_name = \"classification_dataset\" #@param {type: \"string\"}\n","#@markdown - Please make sure to compress your dataset folder to **.zip** file before uploading!\n","#@markdown - The name of the .zip file should be the same as for the dataset folder.\n","\n","from pathlib import Path\n","import zipfile\n","from google.colab import files\n","\n","dataset_location = f\"/content/yolov5/{dataset_name}\"\n","\n","uploaded = files.upload()\n","\n","if len(list(zipfile.Path(f\"{dataset_name}.zip\").iterdir())) > 1:\n","  !unzip -uq {dataset_name}.zip -d {dataset_location}\n","else:\n","  !unzip -uq {dataset_name}.zip -d /content/yolov5\n","%rm {dataset_name}.zip\n","\n","if Path(f\"{dataset_location}/valid\").exists():\n","  Path(f\"{dataset_location}/valid\").rename(f\"{dataset_location}/val\")\n","\n","print(f\"\\nLocation of dataset: {dataset_location}\")\n","print(f\"\\nTotal number of images: {len(list(Path(dataset_location).glob('**/*.jpg')))}\")\n","\n","if Path(f\"{dataset_location}/train\").exists():\n","  classes_train = sorted(list(Path(f\"{dataset_location}/train\").glob(\"*\")))\n","  print(f\"\\nNumber of classes: {len(classes_train)}\")\n","  print(f\"\\nNumber of training images: {len(list(Path(f'{dataset_location}/train').glob('**/*.jpg')))}\")\n","  print(\"Number of images per class in the train split:\")\n","  print(\"\\n\".join(f\"{c.name}: {len(list((Path(c).glob('*.jpg'))))}\" for c in classes_train))\n","else:\n","  classes = sorted(list(Path(dataset_location).glob(\"*\")))\n","  print(f\"\\nNumber of classes: {len(classes)}\")\n","  print(\"\\nNumber of images per class:\")\n","  print(\"\\n\".join(f\"{c.name}: {len(list((Path(c).glob('*.jpg'))))}\" for c in classes))\n","if Path(f\"{dataset_location}/val\").exists():\n","  classes_val = sorted(list(Path(f\"{dataset_location}/val\").glob(\"*\")))\n","  print(f\"\\nNumber of validation images: {len(list(Path(f'{dataset_location}/val').glob('**/*.jpg')))}\")\n","  print(\"Number of images per class in the val split:\")\n","  print(\"\\n\".join(f\"{c.name}: {len(list((Path(c).glob('*.jpg'))))}\" for c in classes_val))\n","if Path(f\"{dataset_location}/test\").exists():\n","  classes_test = sorted(list(Path(f\"{dataset_location}/test\").glob(\"*\")))\n","  print(f\"\\nNumber of test images: {len(list(Path(f'{dataset_location}/test').glob('**/*.jpg')))}\")\n","  print(\"Number of images per class in the test split:\")\n","  print(\"\\n\".join(f\"{c.name}: {len(list((Path(c).glob('*.jpg'))))}\" for c in classes_test))"]},{"cell_type":"markdown","metadata":{"id":"g41kdogczKts"},"source":["## Upload dataset from Roboflow\n","\n","If you are not sure how to export your dataset, check the [Roboflow docs](https://docs.roboflow.com/exporting-data)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aRSmjO9MQRVq"},"outputs":[],"source":["%pip install -q roboflow"]},{"cell_type":"markdown","metadata":{"id":"8cqOYoopQx-U"},"source":["**Copy only the last three lines of your Download Code and insert them in the next code cell:**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vvQNa-Extuw_"},"outputs":[],"source":["from pathlib import Path\n","from roboflow import Roboflow\n","\n","%cd /content/yolov5\n","\n","### Paste your Download Code here:\n","rf = Roboflow(api_key=\"XXXXXXXXXXXXXXXXXXXX\")\n","project = rf.workspace(\"maximilian-sittinger\").project(\"insect_detect_classification_v2\")\n","dataset = project.version(1).download(\"folder\")\n","###\n","\n","dataset_location = dataset.location\n","\n","if Path(f\"{dataset_location}/valid\").exists():\n","  Path(f\"{dataset_location}/valid\").rename(f\"{dataset_location}/val\")\n","\n","print(f\"\\nLocation of dataset: {dataset_location}\")\n","print(f\"\\nTotal number of images: {len(list(Path(dataset_location).glob('**/*.jpg')))}\")\n","\n","if Path(f\"{dataset_location}/train\").exists():\n","  classes_train = sorted(list(Path(f\"{dataset_location}/train\").glob(\"*\")))\n","  print(f\"\\nNumber of classes: {len(classes_train)}\")\n","  print(f\"\\nNumber of training images: {len(list(Path(f'{dataset_location}/train').glob('**/*.jpg')))}\")\n","  print(\"Number of images per class in the train split:\")\n","  print(\"\\n\".join(f\"{c.name}: {len(list((Path(c).glob('*.jpg'))))}\" for c in classes_train))\n","else:\n","  classes = sorted(list(Path(dataset_location).glob(\"*\")))\n","  print(f\"\\nNumber of classes: {len(classes)}\")\n","  print(\"\\nNumber of images per class:\")\n","  print(\"\\n\".join(f\"{c.name}: {len(list((Path(c).glob('*.jpg'))))}\" for c in classes))\n","if Path(f\"{dataset_location}/val\").exists():\n","  classes_val = sorted(list(Path(f\"{dataset_location}/val\").glob(\"*\")))\n","  print(f\"\\nNumber of validation images: {len(list(Path(f'{dataset_location}/val').glob('**/*.jpg')))}\")\n","  print(\"Number of images per class in the val split:\")\n","  print(\"\\n\".join(f\"{c.name}: {len(list((Path(c).glob('*.jpg'))))}\" for c in classes_val))\n","if Path(f\"{dataset_location}/test\").exists():\n","  classes_test = sorted(list(Path(f\"{dataset_location}/test\").glob(\"*\")))\n","  print(f\"\\nNumber of test images: {len(list(Path(f'{dataset_location}/test').glob('**/*.jpg')))}\")\n","  print(\"Number of images per class in the test split:\")\n","  print(\"\\n\".join(f\"{c.name}: {len(list((Path(c).glob('*.jpg'))))}\" for c in classes_test))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vkr0vBcOlT-t"},"outputs":[],"source":["#@title ## Optional: Split dataset into train/val/test subsets {display-mode: \"form\"}\n","\n","train_ratio = 0.7 #@param {type:\"slider\", min:0, max:1.0, step:0.1}\n","val_ratio = 0.2 #@param {type:\"slider\", min:0, max:1.0, step:0.1}\n","test_ratio = 0.1 #@param {type:\"slider\", min:0, max:1.0, step:0.1}\n","#@markdown ---\n","\n","#@markdown Set random seed for shuffling of the images before splitting:\n","random_seed = 1 #@param {type: \"integer\"}\n","#@markdown **Use the same seed to make splits reproducible. Change the seed to generate a new split.**\n","#@markdown > More info about other options: [`split-folders`](https://github.com/jfilter/split-folders#usage)\n","\n","%pip install -q split-folders\n","\n","from pathlib import Path\n","import splitfolders\n","\n","if Path(f\"{dataset_location}/train\").exists():\n","  print(\"Train split of your dataset already exists!\")\n","else:\n","  input_dir = dataset_location\n","  output_dir = f\"{input_dir}_split\"\n","  dataset_location = Path(output_dir)\n","\n","  splitfolders.ratio(input_dir, output_dir, seed=random_seed, ratio=(train_ratio, val_ratio, test_ratio))\n","\n","  print(f\"\\nNew location of dataset after split: {dataset_location}\")\n","  print(f\"\\nTotal number of images: {len(list(Path(dataset_location).glob('**/*.jpg')))}\")\n","\n","  classes_train = sorted(list(Path(f\"{dataset_location}/train\").glob(\"*\")))\n","  print(f\"\\nNumber of classes: {len(classes_train)}\")\n","  print(f\"\\nNumber of training images: {len(list(Path(f'{dataset_location}/train').glob('**/*.jpg')))}\")\n","  print(\"Number of images per class in the train split:\")\n","  print(\"\\n\".join(f\"{c.name}: {len(list((Path(c).glob('*.jpg'))))}\" for c in classes_train))\n","  if Path(f\"{dataset_location}/val\").exists():\n","    classes_val = sorted(list(Path(f\"{dataset_location}/val\").glob(\"*\")))\n","    print(f\"\\nNumber of validation images: {len(list(Path(f'{dataset_location}/val').glob('**/*.jpg')))}\")\n","    print(\"Number of images per class in the val split:\")\n","    print(\"\\n\".join(f\"{c.name}: {len(list((Path(c).glob('*.jpg'))))}\" for c in classes_val))\n","  if Path(f\"{dataset_location}/test\").exists():\n","    classes_test = sorted(list(Path(f\"{dataset_location}/test\").glob(\"*\")))\n","    print(f\"\\nNumber of test images: {len(list(Path(f'{dataset_location}/test').glob('**/*.jpg')))}\")\n","    print(\"Number of images per class in the test split:\")\n","    print(\"\\n\".join(f\"{c.name}: {len(list((Path(c).glob('*.jpg'))))}\" for c in classes_test))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aVZgGZtc8skH"},"outputs":[],"source":["#@title ## Optional: Calculate metrics of your image dataset {display-mode: \"form\"}\n","\n","#@markdown - In our experiments, upscaling of most images in the dataset led to better training\n","#@markdown   results compared to downscaling of the images with the `cv2.INTER_LINEAR` method that\n","#@markdown   is used by default during YOLOv5 image preprocessing\n","#@markdown   ([comparison of OpenCV interpolation algorithms](https://web.archive.org/web/20190424180810/http://tanbakuchi.com/posts/comparison-of-openv-interpolation-algorithms/)).\n","#@markdown   You can use the 90th percentile of the image sizes (divisible by 32) in your dataset\n","#@markdown   as reference point to set the input image size for model training in the next step.\n","#@markdown > Compare models trained with different image sizes to find the best accuracy for your dataset!\n","\n","from pathlib import Path\n","from statistics import mean, median\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from PIL import Image\n","\n","images = list(Path(f\"{dataset_location}\").glob(\"**/*.jpg\"))\n","print(f\"Found {len(images)} .jpg images in the dataset folder.\\n\")\n","\n","img_widths = []\n","img_heights = []\n","\n","for img in images:\n","  with Image.open(img) as im:\n","    img_widths.append(im.width)\n","    img_heights.append(im.height)\n","\n","print(f\"Mean image width:    {round(mean(img_widths))} (min: {min(img_widths)} / max: {max(img_widths)})\")\n","print(f\"Mean image height:   {round(mean(img_heights))} (min: {min(img_heights)} / max: {max(img_heights)})\\n\")\n","print(f\"Median image width:  {round(median(img_widths))}\")\n","print(f\"Median image height: {round(median(img_heights))}\\n\")\n","\n","p90_image_size = round(mean([np.percentile(img_widths, 90), np.percentile(img_heights, 90)]))\n","print(f\"Mean 90th percentile of image width/height: {p90_image_size}\")\n","print(f\"Recommended image_size for training:        {int(32 * round(p90_image_size / 32))}\\n\")\n","\n","plt.scatter(img_widths, img_heights, marker=\".\", alpha=0.3, edgecolors=\"black\")\n","plt.axhline(y=mean(img_heights), color=\"green\", linestyle=\"--\", label=\"Mean\")\n","plt.axvline(x=mean(img_widths), color=\"green\", linestyle=\"--\")\n","plt.axhline(y=np.percentile(img_heights, 90), color=\"red\", linestyle=\"--\", label=\"90th percentile\")\n","plt.axvline(x=np.percentile(img_widths, 90), color=\"red\", linestyle=\"--\")\n","plt.legend()\n","plt.rcParams[\"axes.axisbelow\"] = True\n","plt.grid(color=\"gray\", linewidth=0.5, alpha=0.2)\n","plt.title(\"Distribution of image width/height in the dataset\")\n","plt.xlabel(\"Image width\")\n","plt.ylabel(\"Image height\")\n","#plt.gcf().set_dpi(300) # higher quality for saving to .png\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"nnn4pSbI6eTv"},"source":["# Model training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8IxTKp7CjuOA"},"outputs":[],"source":["#@title ## Optional: Select external logger {display-mode: \"form\"}\n","\n","logger = \"Weights&Biases\" #@param [\"Weights&Biases\", \"Comet\", \"ClearML\"]\n","\n","#@markdown > More info:\n","#@markdown - [Weights & Biases](https://docs.wandb.ai/guides/integrations/yolov5)\n","#@markdown - [Comet](https://github.com/ultralytics/yolov5/tree/master/utils/loggers/comet)\n","#@markdown - [ClearML](https://github.com/ultralytics/yolov5/tree/master/utils/loggers/clearml)\n","\n","if logger == \"Weights&Biases\":\n","  %pip install -q wandb\n","  import wandb\n","  wandb.login()\n","elif logger == \"Comet\":\n","  %pip install -q comet_ml\n","  import comet_ml\n","  comet_ml.init()\n","elif logger == \"ClearML\":\n","  %pip install -q clearml\n","  import clearml\n","  clearml.browser_login()"]},{"cell_type":"markdown","metadata":{"id":"U4t4JhGYGOcr"},"source":["## Tensorboard logger\n","\n","> If you are using Firefox, **disable Enhanced Tracking Protection** for this website (click on the shield to the left of the address bar) for the Tensorboard logger to work correctly!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lYCUyGITGU6j"},"outputs":[],"source":["%load_ext tensorboard\n","%tensorboard --logdir /content/yolov5/runs/train-cls"]},{"cell_type":"markdown","metadata":{"id":"7t6PUcz3SsEy"},"source":["## Train image classification model\n","\n","- `--name` name of the training run\n","- `--imgsz` input image size\n","- `--batch` specify batch size (recommended: 64)\n","- `--epochs` set the number of training [epochs](https://machine-learning.paperspace.com/wiki/epoch) (recommended: 10-30)\n","- `--data` path to dataset folder\n","- `--model` specify the pretrained [classification model](https://github.com/ultralytics/yolov5#classification) (recommended: EfficientNet-B0)\n","- `--cache` cache images in RAM for faster training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qAYNJg9M7sg3"},"outputs":[],"source":["training_run_name = \"EfficientNet-B0_128_batch64_epochs20\" #@param {type: \"string\"}\n","#@markdown Add UTC timestamp in front of training run name:\n","add_timestamp = True #@param {type:\"boolean\"}\n","#@markdown ---\n","\n","image_size = 128 #@param {type:\"slider\", min:32, max:224, step:32}\n","batch_size = 64 #@param {type:\"slider\", min:32, max:128, step:32}\n","number_epochs = 20 #@param {type:\"slider\", min:5, max:50, step:5}\n","model = \"efficientnet_b0.pt\" #@param [\"yolov5n-cls.pt\", \"yolov5s-cls.pt\", \"yolov5m-cls.pt\", \"yolov5l-cls.pt\", \"yolov5x-cls.pt\", \"resnet18.pt\", \"resnet34.pt\", \"resnet50.pt\", \"resnet101.pt\", \"efficientnet_b0.pt\", \"efficientnet_b1.pt\", \"efficientnet_b2.pt\", \"efficientnet_b3.pt\"]\n","\n","if add_timestamp:\n","  from datetime import datetime\n","  utc_timestamp = datetime.now().strftime(\"%Y%m%d_%H-%M\")\n","  train_run_name = f\"{utc_timestamp}_{training_run_name}\"\n","else:\n","  train_run_name = training_run_name\n","\n","%cd /content/yolov5\n","\n","!python classify/train.py \\\n","--name {train_run_name} \\\n","--imgsz {image_size} \\\n","--batch {batch_size} \\\n","--epochs {number_epochs} \\\n","--data {dataset_location} \\\n","--model {model} \\\n","--cache"]},{"cell_type":"markdown","metadata":{"id":"KEVQhYGc2I6m"},"source":["### Export trained model weights to ONNX format for faster CPU inference"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sNcRoQ_H2K-Z"},"outputs":[],"source":["%cd /content/yolov5\n","\n","!python export.py \\\n","--weights runs/train-cls/{train_run_name}/weights/best.pt \\\n","--imgsz {image_size} \\\n","--include onnx \\\n","--simplify \\\n","--device 0 # use \"--device cpu\" if not connected to GPU runtime"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h90_4rFQx0mp"},"outputs":[],"source":["#@title ## Export to Google Drive or Download training results {display-mode: \"form\"}\n","\n","training_results = \"Export_Google_Drive\" #@param [\"Export_Google_Drive\", \"Download\"]\n","#@markdown ---\n","\n","#@markdown ### Path for saving training results in Google Drive:\n","GDrive_save_path = \"/content/drive/MyDrive/Training_results/YOLOv5-cls\" #@param {type: \"string\"}\n","\n","if training_results == \"Export_Google_Drive\":\n","  print(\"Exporting training results to Google Drive...\\n\")\n","  !rsync -ah --mkpath --info=progress2 --no-i-r /content/yolov5/runs/train-cls/{train_run_name} {GDrive_save_path}\n","  print(\"\\nTraining results were successfully exported!\")\n","elif training_results == \"Download\":\n","  from google.colab import files\n","  %cd /content/yolov5/runs/train-cls\n","  !zip -rq {train_run_name}.zip {train_run_name}\n","  %cd -\n","  files.download(f\"/content/yolov5/runs/train-cls/{train_run_name}.zip\")"]},{"cell_type":"markdown","metadata":{"id":"gh_TdWPfR55N"},"source":["# Model validation\n","\n","Test the classification accuracy of your model on the validation and/or test dataset.\n","\n","> Change the weights from `best.onnx` to `best.pt` if you want to use the original PyTorch model for validation."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X2EUMNsutZA5"},"outputs":[],"source":["task = \"val\" #@param [\"val\", \"test\"]\n","#@markdown > Use `task: test` to validate on the dataset test split.\n","\n","from IPython.display import Image, display\n","\n","val_run_name = f\"{train_run_name}_validate_{task}\"\n","\n","%cd /content/yolov5\n","\n","!python classify/val.py \\\n","--name {val_run_name} \\\n","--weights runs/train-cls/{train_run_name}/weights/best.onnx \\\n","--data {dataset_location} \\\n","--imgsz {image_size} \\\n","--task {task}\n","\n","print(\"\\n\")\n","display(Image(f\"runs/val-cls/{val_run_name}/confusion_matrix_{task}.png\", width=800))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JBnO-orX24YJ"},"outputs":[],"source":["#@title ## Export to Google Drive or Download validation results {display-mode: \"form\"}\n","\n","validation_results = \"Export_Google_Drive\" #@param [\"Export_Google_Drive\", \"Download\"]\n","#@markdown ---\n","\n","#@markdown ### Path for saving validation results in Google Drive:\n","GDrive_save_path = \"/content/drive/MyDrive/Training_results/YOLOv5-cls\" #@param {type: \"string\"}\n","\n","if validation_results == \"Export_Google_Drive\":\n","  print(\"Exporting validation results to Google Drive...\\n\")\n","  !rsync -ah --mkpath --info=progress2 --no-i-r /content/yolov5/runs/val-cls/{val_run_name} {GDrive_save_path}/{train_run_name}\n","  print(\"\\nValidation results were successfully exported!\")\n","elif validation_results == \"Download\":\n","  from google.colab import files\n","  %cd /content/yolov5/runs/val-cls\n","  !zip -rq {val_run_name}.zip {val_run_name}\n","  %cd -\n","  files.download(f\"/content/yolov5/runs/val-cls/{val_run_name}.zip\")"]},{"cell_type":"markdown","metadata":{"id":"txk2Jl4nR8Bc"},"source":["# Model inference\n","\n","Use your model to classify the images in the dataset test split.\n","\n","> Change the weights from `best.onnx` to `best.pt` if you want to use the original PyTorch model for prediction."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q3mTwl1IuCDh"},"outputs":[],"source":["from IPython.display import Image, display\n","\n","pred_run_name = f\"{train_run_name}_predict\"\n","\n","%cd /content/yolov5\n","\n","!python classify/predict.py \\\n","--name {pred_run_name} \\\n","--weights runs/train-cls/{train_run_name}/weights/best.onnx \\\n","--source {dataset_location}/test/*/*/ \\\n","--imgsz {image_size}\n","\n","display(Image(f\"runs/predict-cls/{pred_run_name}/results/top1_prob.png\", width=800))\n","print(\"\\n\")\n","display(Image(f\"runs/predict-cls/{pred_run_name}/results/top1_prob_mean.png\", width=800))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5eoHLUq9S0KC"},"outputs":[],"source":["#@title ## Export to Google Drive or Download inference results {display-mode: \"form\"}\n","\n","inference_results = \"Export_Google_Drive\" #@param [\"Export_Google_Drive\", \"Download\"]\n","#@markdown Include images with inference results (top 1 class + probability):\n","include_images = False #@param {type:\"boolean\"}\n","#@markdown ---\n","\n","#@markdown ### Path for saving inference results in Google Drive:\n","GDrive_save_path = \"/content/drive/MyDrive/Training_results/YOLOv5-cls\" #@param {type: \"string\"}\n","\n","if include_images:\n","  %cd /content/yolov5/runs/predict-cls\n","  !zip -rq {pred_run_name}.zip {pred_run_name}\n","  %cd -\n","\n","if inference_results == \"Export_Google_Drive\":\n","  print(\"\\nExporting inference results to Google Drive...\\n\")\n","  if include_images:\n","    !rsync -ah --mkpath --info=progress2 --no-i-r /content/yolov5/runs/predict-cls/{pred_run_name}.zip {GDrive_save_path}/{train_run_name}\n","  else:\n","    !rsync -ah --mkpath --info=progress2 --no-i-r /content/yolov5/runs/predict-cls/{pred_run_name}/results {GDrive_save_path}/{train_run_name}/{pred_run_name}\n","  print(\"\\nInference results were successfully exported!\")\n","elif inference_results == \"Download\":\n","  from google.colab import files\n","  if include_images:\n","    files.download(f\"/content/yolov5/runs/predict-cls/{pred_run_name}.zip\")\n","  else:\n","    %cd /content/yolov5/runs/predict-cls\n","    !zip -rq {pred_run_name}.zip {pred_run_name}/results\n","    %cd -\n","    files.download(f\"/content/yolov5/runs/predict-cls/{pred_run_name}.zip\")"]},{"cell_type":"markdown","metadata":{"id":"zJUgjlS_BYBk"},"source":["## Show inference results on test images"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lHTxDqIF-x_q"},"outputs":[],"source":["from pathlib import Path\n","from IPython.display import Image, display\n","\n","for img in Path(f\"/content/yolov5/runs/predict-cls/{pred_run_name}\").glob(\"*.jpg\"):\n","  display(Image(img))\n","  print(\"\\n\")"]},{"cell_type":"markdown","metadata":{"id":"5MvksenuUeOS"},"source":["# Model deployment\n","\n","That's it! You trained an image classification model on your custom dataset with [YOLOv5](https://github.com/ultralytics/yolov5#classification) and exported it to ONNX format for faster CPU inference.\n","\n","> To deploy the classification model on your local PC, check out the deployment instructions in the [**Insect Detect Docs**](https://maxsitt.github.io/insect-detect-docs/deployment/classification/)."]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","private_outputs":true,"provenance":[{"file_id":"https://github.com/maxsitt/insect-detect-ml/blob/main/notebooks/YOLOv5_classification_training.ipynb","timestamp":1691509866499},{"file_id":"https://github.com/maxsitt/insect-detect-ml/blob/main/notebooks/YOLOv5_classification_training.ipynb","timestamp":1690495870983},{"file_id":"1kFY1gEzCpz63EIowUvWoG5LI2dQyjvgk","timestamp":1689769699218},{"file_id":"1v0KZkTR1VX6WF1BR-QJNzbyCoecRzEUz","timestamp":1689329325559},{"file_id":"1Ew8DM2y7Fdswde-FmcF2M5qu0nSXh4t5","timestamp":1689063499211},{"file_id":"https://github.com/maxsitt/insect-detect-ml/blob/main/notebooks/YOLOv5_classification_training.ipynb","timestamp":1687190674701},{"file_id":"1qbts-t2tpeTOuXGtfR47jilzlD9Fyuf0","timestamp":1671662208154}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.4"},"vscode":{"interpreter":{"hash":"baf2788c67905bf5eabce425833f665485fde887eca8cd7474f373ca3e9af677"}}},"nbformat":4,"nbformat_minor":0}
