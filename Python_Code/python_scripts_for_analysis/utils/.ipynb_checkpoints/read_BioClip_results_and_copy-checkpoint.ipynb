{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72010975-3b6d-405d-b7a2-13c66fdd7934",
   "metadata": {},
   "source": [
    "This notebook reads the output csv of the apollo toilet app. It then copies a user defined number of random images that were labeled as a specific label from an input folder to an output folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2244a0dc-3403-4b46-b4bc-7d076b705cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b4ee4ad-7360-4670-8e07-713d60c3ffbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_duplicates(csv_file):\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(csv_file)\n",
    "    \n",
    "    # Extract the basename from the 'Filename' column (ignores path)\n",
    "    df['basename'] = df['Filename'].apply(lambda x: os.path.basename(x))\n",
    "    \n",
    "    # Check for duplicate basenames\n",
    "    duplicate_rows = df['basename'].duplicated()\n",
    "    num_duplicates = duplicate_rows.sum()  # Count duplicates\n",
    "    \n",
    "    if num_duplicates > 0:\n",
    "        print(f\"The CSV file contains {num_duplicates} duplicate filenames. Duplicates were deleted from df!!\")\n",
    "    else:\n",
    "        print(\"The CSV file has no duplicate filenames.\")\n",
    "    \n",
    "    # Remove the duplicate rows based on 'basename' column\n",
    "    filtered_df = df[~duplicate_rows]  # Exclude the rows with duplicates\n",
    "    \n",
    "    # Drop the 'basename' column as it's no longer needed\n",
    "    filtered_df = filtered_df.drop(columns=['basename'])\n",
    "    \n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f47cd830-ca70-49d0-a9a4-157e0ada261c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_csv_and_copy_images(csv_file, input_folder, output_folder, num_files, filter_for):\n",
    "    df = check_for_duplicates(csv_file)\n",
    "    # Read the CSV file\n",
    "    #df = pd.read_csv(csv_file)\n",
    "    \n",
    "    # Filter rows where 'Response' column is 'Correct'\n",
    "    filtered_df = df[df['Response'] == filter_for]\n",
    "    \n",
    "    # Check if there are enough rows to sample\n",
    "    if len(filtered_df) < num_files:\n",
    "        raise ValueError(f\"Not enough {filter_for} entries to sample {num_files} rows. Only {len(filtered_df)} entries found...\")\n",
    "    \n",
    "    # Randomly select the specified number of rows\n",
    "    sampled_df = filtered_df.sample(n=num_files)\n",
    "    \n",
    "    # Ensure the output folder exists\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    # Process each sampled row\n",
    "    for _, row in sampled_df.iterrows():\n",
    "        # Extract the filename from the path\n",
    "        full_path = row['Filename']\n",
    "        filename = os.path.basename(full_path)\n",
    "        \n",
    "        # Check if the corresponding file exists in the input folder\n",
    "        input_file_path = os.path.join(input_folder, filename)\n",
    "        if os.path.exists(input_file_path):\n",
    "            # Copy the file to the output folder\n",
    "            shutil.copy(input_file_path, output_folder)\n",
    "        else:\n",
    "            print(f\"Warning: File {filename} not found in the input folder.\")\n",
    "    \n",
    "    print(f\"Successfully copied {num_files} files to {output_folder}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8aa534e-4e86-4b8e-b7cd-2fbc2f9d0717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the file paths and parameters\n",
    "csv_file_path = \"/Volumes/T7_Shield/Diopsis_Cameras/App/user_checked_predictions_24_01_25.csv\"  # Path to the CSV file\n",
    "input_images_folder = \"/Volumes/T7_Shield/Diopsis_Cameras/RESULTS_2024/all_crops/all_crops\"  # Path to the folder containing images\n",
    "output_images_folder = \"/Volumes/T7_Shield/Diopsis_Cameras/Train_Test_datasets/Apollo_dirt_classifier/dirt_test_new/no_insect\"  # Path to the folder to save the copied images\n",
    "number_of_files = 1000  # Number of files to randomly select and copy\n",
    "filter_for = \"Dirt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee09826e-60f9-401b-aa12-e3729a28150f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The CSV file contains 394 duplicate filenames. Duplicates were deleted from df!!\n",
      "Successfully copied 1000 files to /Volumes/T7_Shield/Diopsis_Cameras/Train_Test_datasets/Apollo_dirt_classifier/dirt_test_new/no_insect\n"
     ]
    }
   ],
   "source": [
    "process_csv_and_copy_images(\n",
    "    csv_file=csv_file_path,\n",
    "    input_folder=input_images_folder,\n",
    "    output_folder=output_images_folder,\n",
    "    num_files=number_of_files,\n",
    "    filter_for = filter_for\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b338e5-af1e-4514-a630-1cfe7141a8af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
