{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85d49a69-43d4-4166-a85f-b7c0eb31c393",
   "metadata": {},
   "source": [
    "This notebook contains the pipeline to analyze insect images from a single image folder "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b20e68-aeb0-4a86-b26a-64fcf739dab8",
   "metadata": {},
   "source": [
    "##### Implemented:\n",
    "- Looks at the brightness of each image. If the image is to dark, it will not be analyzed and moved to the `dark_frames` folder.\n",
    "- Identifies insects in the image with GroundinDino, defines a bounding-box and saves a cropped image of the insect in the `cropped` folder.\n",
    "- Checks whether the detection has already appeared on one of the last images to prevent saving a cropped image of the same individuum over and over. The tracker from AMT (Automatic Moth Trap: https://stangeia.hobern.net/autonomous-moth-trap-image-pipeline/) was implemented for this task.\n",
    "- AMT also includes funtionality that attempts to optimize the cropped image of an individuum by saving a new version of the insect if it is sharper than the old one.\n",
    "- The code quickly checks the size of each detection. If it is unrealistically big, the crop is saved in the `potentially_faulty` folder.\n",
    "- Saves the raw images with the tracking visualized on them in another subfolder called `detection_drawings` - if activated.\n",
    "- Goes through all the cropped images in `cropped` again and sorts out any that are not recognized as an insect. Moves these images into the `potentially_faulty` folder. A custom trained algorithm (Apollo Environment, inference_dirt_classifier.py) is responsible for this. Room for improvement here through better training data.\n",
    "- Classifies the cropped images using the InsectDetect classifier (https://github.com/maxsitt/insect-detect/tree/main?tab=readme-ov-file), ApolloNet - a self trained neural Network, or BioClip (https://imageomics.github.io/bioclip/) (https://github.com/Imageomics/pybioclip)\n",
    "- Measures the length of the insects in the cropped images using a custom trained version of the SLEAP pipeline (https://sleap.ai/tutorials/initial-training.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76eae0bf-9adf-4a31-9f83-f054f5b35c23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The envs directory is: C:\\Users\\rolingni\\AppData\\Local\\anaconda3\\envs\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForMaskGeneration, AutoProcessor, pipeline\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual, Layout\n",
    "from PIL import Image, ImageDraw, ImageFont, PngImagePlugin\n",
    "from typing import Any, List, Dict, Optional, Union, Tuple\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from bioclip import TreeOfLifeClassifier, Rank\n",
    "from IPython.display import clear_output\n",
    "from ipyfilechooser import FileChooser\n",
    "import matplotlib.patches as patches\n",
    "import plotly.graph_objects as go\n",
    "from dataclasses import dataclass\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import ipywidgets as widgets\n",
    "import plotly.express as px\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import subprocess\n",
    "import warnings\n",
    "import requests\n",
    "import random\n",
    "import torch\n",
    "import glob\n",
    "import json\n",
    "import math\n",
    "import time\n",
    "import csv\n",
    "import cv2\n",
    "import os\n",
    "import re\n",
    "\n",
    "### Selfmade\n",
    "from FunktionenZumImportieren.helper_funktionen_clean import *\n",
    "from FunktionenZumImportieren.settings_widgets import *\n",
    "from AMT_functions.colors import reportcolors\n",
    "from AMT_functions.amt_tracker import *\n",
    "\n",
    "warnings.simplefilter(action='always', category=UserWarning)\n",
    "warnings.formatwarning = custom_warning_format\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "envs_path = get_local_env_path()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106398b3-e2b8-446b-986c-4772b972c17a",
   "metadata": {},
   "source": [
    "#### Define settings variables\n",
    "##### Variables include:\n",
    "- labels: prompt that specifies what GroundingDino searches in the images. Leave as `insect`.\n",
    "- threshold: a threshold that tells GroundingDino whether or not to reject a detection or to accept it. Lower values means more detections but also more faulty detections.\n",
    "- buffer: specifies the amount of border space in the cropped images. `15` is a good value usually.\n",
    "- pixel_scale: how many pixels are in a cm of image on the current camera. Necessary for outputing mm readings of the insect length. Input `10` if you want to output the length in pixel.\n",
    "- start_image: The name of a picture can be specified here (with extention). The code will then skip all the files before. Leave empty if you want to analyze all the images in the folder. Please leave this empty in the batch analysis pipeline. Might cause issues. If you want to start at the folder of a specific day, then put the folder number at the first spot in the brackets ([___:]) where the big arrow is in the main cell (<---). If you want to start from the 5th folder for some reason, then put in a 4.\n",
    "- save_visualisation: activate if you want a visualisation of the detections and tracks saved in \"detection_drawings\"\n",
    "- rank: Defines the taxonomic rank to which insects should be classified if BioClip is used. Can be set to None. This will make the algorithm classify up to the taxonomic rank that first satisfies the requirement set by certainty_threshold (see the function BioClip_inference in helper_funktionen). Setting rank to None will increase compute time.\n",
    "- image_folder: folder containing the raw images to be analyzed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a978a9-5527-4d7f-8984-348efacf53fe",
   "metadata": {},
   "source": [
    "    WORD OF CAUTION: The 'score' value (in the results) of the BioClip inference will only reflect the certainty for the taxonomic rank up to which the classification went. That means that if you classified up to species level for example, and later in the analysis go up to family level, the score will no longer reflect how certain BioClip is for the family rank. The score always reflects the certainty that is achieved for the taxonomic rank that is stated in the 'highest_classification_rank' column of the results csv. If you first classified to species, but then later decide to switch your analysis to family, you need to run the inference again (ideally with the 'Only_BioClip_inference' notebook in 'utils') in order to obtain the right score values for that taxonomic rank. Otherwise, the 'score' value might appear either way to high or way to low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8aa87b4-35c9-4e94-ab8f-2124fa8d16d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f24ce8fad4f4c4facc15c2fa04728b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Annotation algorithm:', layout=Layout(height='30px', width='50%'), options=(('BioClip', …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2ccd0052dd9481885866e80abdb61f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Checkbox(value=True, description='Perform image classification with BioClip', indent=False)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41d9133211144abe897b08fcab284370",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Taxonomic rank:', index=4, layout=Layout(height='30px', width='50%'), options=(('kingdom…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b3549b6efc54faaa286c77265b55722",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Checkbox(value=False, description='Perform image classification with ApolloNet', indent=False)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "190b181207cc407f8b79785002d2ddc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Checkbox(value=False, description='Perform image classification with InsectDetect', indent=False)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93c1e0a889a6405bb80494cf1a2fc288",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Checkbox(value=False, description='Save visualisations', indent=False)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53be717a5a3642bf9b5ed514eb181118",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', description='If you want to start at a specific image:', layout=Layout(height='30px', width='50…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "get_values = create_interactive_widgets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bea18744-3220-473b-a2d9-c0acca7675df",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"insect\"]\n",
    "threshold = 0.3\n",
    "buffer = 15\n",
    "pixel_scale = 87"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7d31e5b-0c05-4bfd-9854-73d7f2d46a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## On Mac:\n",
    "#image_folder = \"/Users/rentaluser/Downloads/Diopsis_photo_2024_09_03\"\n",
    "\n",
    "## On Windows:\n",
    "image_folder = r\"C:\\Users\\rolingni\\OneDrive - Eidg. Forschungsanstalt WSL\\Bilder\\input_tests\\input_test\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b44ab56-1656-4fd3-91e9-b6bc573bec50",
   "metadata": {},
   "source": [
    "Main pipeline cell. All functionallity is included here\\\n",
    "Carefully monitor the printed details since some errors are not cought, but rather only printed. An error summary will appear at the end, telling you whether an error occured during the whole inference or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0856b814-f7c0-454f-b65f-c2b95dedfac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- GroundingDino runs on cpu ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing image 20240825235447.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:14<00:00,  7.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nr. of detections: 8\n",
      "Analyzing image cropped\n",
      "--------\n",
      "Done detecting all the insects and saving cropped versions.\n",
      "Double-checking for faulty detections...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All crops seem to be insects :)\n",
      "--------\n",
      "Classifying crops now.\n",
      "The tree of life is growing: running BioClip algorithm for image classification now...\n",
      "Classified all images up to family-level\n",
      "--------\n",
      "Done classifying the insects. Measuring body lengths now...\n",
      "Length measurements ran clean\n",
      "--------\n",
      "Done measuring. Annotating all results onto cropped images now...\n",
      "--------\n",
      "Done with everything! No length Errors occured :)\n",
      "Elapsed time: 1.28 minutes \n",
      "Time per Image: 38.45 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "settings = get_values()\n",
    "start_processing = False\n",
    "pixel_scale /= 10\n",
    "blobs = []\n",
    "trails = {}\n",
    "classifier = TreeOfLifeClassifier()\n",
    "tracker = AMTTracker(config)\n",
    "object_detector = load_grounding_dino_model()\n",
    "first_pass = True\n",
    "images = os.listdir(image_folder)\n",
    "images = sorted(images)\n",
    "\n",
    "for image in tqdm(images):\n",
    "    print(\"Analyzing image\", image)\n",
    "    image_arg = os.path.join(image_folder, image)\n",
    "    skip, start_processing = should_image_be_skipped(settings.start_image, start_processing, image_arg, image, image_folder)\n",
    "    if skip:\n",
    "        continue\n",
    "    detections = detect(\n",
    "        object_detector,\n",
    "        image=image_arg,\n",
    "        labels=labels,\n",
    "        threshold=threshold\n",
    "    )\n",
    "    print(\"Nr. of detections:\", len(detections))\n",
    "    image_array = Image.open(image_arg)\n",
    "    image_array = np.array(image_array)\n",
    "    blobs = convert_bounding_boxes(detections, image, image_array, image_folder, buffer)\n",
    "    if first_pass:\n",
    "        tracker.savedois = blobs\n",
    "    new_tracks, _ = tracker.managetracks(tracker.savedois, blobs, first_pass)\n",
    "    tracker.savedois = new_tracks\n",
    "    first_pass = False\n",
    "\n",
    "    if settings.save_visualisation:\n",
    "        plot_tracks_and_detections(image_array, new_tracks, image)\n",
    "\n",
    "print(\"--------\")\n",
    "\n",
    "## Classifying the cropped images\n",
    "print(\"Done detecting all the insects and saving cropped versions.\")\n",
    "\n",
    "print(\"Double-checking for faulty detections...\")\n",
    "Apollo_input_path = os.path.join(image_folder, \"cropped\")\n",
    "Apollo_script_path = os.path.join(envs_path, \"ApolloNet\", \"Intro-to-CV-for-Ecologists-main\", \"inference_dirt_classifier.py\")\n",
    "Apollo_command = f'conda run -n ApolloNet python \"{Apollo_script_path}\" \"{Apollo_input_path}\"'\n",
    "Apollo = subprocess.run(Apollo_command, shell=True, capture_output=True, text=True)\n",
    "#print(Apollo.stdout)\n",
    "if Apollo.returncode != 0:\n",
    "    print(Apollo.stderr)\n",
    "else:\n",
    "    matches = re.search(r\"HERE (\\d+)\", Apollo.stdout)\n",
    "    matches = int(matches.group(1))\n",
    "    if matches > 0:\n",
    "        print(f\"Found {matches} potentially faulty crops. Moved them to the potentially_faulty folder.\\n--------\")\n",
    "    else:\n",
    "        print(\"All crops seem to be insects :)\\n--------\")\n",
    "\n",
    "\n",
    "print(\"Classifying crops now.\")        \n",
    "\n",
    "if settings.InsectDetect:\n",
    "    print(\"InsectDetect classifier running...\")\n",
    "    os.chdir(os.path.join(envs_path, \"InsectDetectSAM\", \"yolov5-cls\"))\n",
    "    print(\"Working directory is now:\", os.getcwd())\n",
    "    InsectDetect_input_path = os.path.join(image_folder, \"cropped\")\n",
    "    !python classify/predict.py --project {image_folder} --name results --source {InsectDetect_input_path} --weights \"insect-detect-ml-main/models/efficientnet-b0_imgsz128.onnx\" --img 128 --sort-top1 --sort-prob --concat-csv\n",
    "    print(\"--------\")\n",
    "\n",
    "if settings.ApolloNet:\n",
    "    print(\"Performing ApolloNet Classification :)\")\n",
    "    Apollo_input_path = os.path.join(image_folder, \"cropped\")\n",
    "    Apollo_script_path = os.path.join(envs_path, \"ApolloNet\", \"Intro-to-CV-for-Ecologists-main\", \"inference.py\")\n",
    "    Apollo_command = f'conda run -n ApolloNet python \"{Apollo_script_path}\" \"{Apollo_input_path}\"'\n",
    "    Apollo = subprocess.run(Apollo_command, shell=True, capture_output=True, text=True)\n",
    "    if Apollo.returncode != 0:\n",
    "        print(Apollo.stderr)\n",
    "    else:\n",
    "        print(\"ApolloNet ran clean\\n--------\")\n",
    "\n",
    "if settings.BioClip:\n",
    "    print(\"The tree of life is growing: running BioClip algorithm for image classification now...\")\n",
    "    BioClip_input_path = os.path.join(image_folder, \"cropped\")\n",
    "    crops_for_BioClip = glob.glob(os.path.join(BioClip_input_path, '*'))\n",
    "    crops_for_BioClip = sorted([item for item in crops_for_BioClip if os.path.isfile(item)])\n",
    "    BioClip_predictions = BioClip_inference(classifier, crops_for_BioClip, settings.rank, certainty_threshold = 0.45)\n",
    "    if len(BioClip_predictions) > 0:\n",
    "        clean_predictions = process_BioClip_predictions(BioClip_predictions, image_folder, settings.rank)\n",
    "    print(\"--------\")\n",
    "    \n",
    "\n",
    "print(\"Done classifying the insects. Measuring body lengths now...\")\n",
    "length_input_path = os.path.join(image_folder, \"cropped\")\n",
    "length_script_path = os.path.join(envs_path, \"sleap\", \"body_length_inference_folder.py\")\n",
    "command = f'conda run -n sleap python \"{length_script_path}\" \"{length_input_path}\" \"{pixel_scale}\"'\n",
    "ran_clean = subprocess.run(command, shell=True, capture_output=True, text=True)\n",
    "if ran_clean.returncode != 0:\n",
    "    print(f\"stdout = {ran_clean.stdout}\")\n",
    "    traceback_index = ran_clean.stderr.find(\"Traceback\")\n",
    "    print(ran_clean.stderr[traceback_index:])\n",
    "else:\n",
    "    print(\"Length measurements ran clean\\n--------\")\n",
    "\n",
    "merge_result_csvs(image_folder)\n",
    "    \n",
    "\n",
    "print(\"Done measuring. Annotating all results onto cropped images now...\")\n",
    "results_csv = get_classification_results_csv(image_folder, settings.annotation_algorithm)\n",
    "input_folder = os.path.join(image_folder, \"cropped_and_annotated\")\n",
    "length_csv_file_path = os.path.join(image_folder, \"results\", \"body_length_results.csv\")\n",
    "\n",
    "if results_csv is not None:\n",
    "    annotate_classifications(classification_results_csv = results_csv, body_length_csv = length_csv_file_path,\n",
    "                             cropped_images_folder = input_folder, image_folder = image_folder, pixel_scale = pixel_scale,\n",
    "                            annotation_algorithm = settings.annotation_algorithm)\n",
    "\n",
    "print(\"--------\")\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print(f\"Done with everything! No length Errors occured :)\\nElapsed time: {elapsed_time/60:.2f} minutes \\nTime per Image: {elapsed_time/len(images):.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48b16dc-62eb-4eb3-8d8a-cb9b62fe3d69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
